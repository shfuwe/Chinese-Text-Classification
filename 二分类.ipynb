{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device0 = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")#训练集gpu\n",
    "device1 = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")#测试集gpu\n",
    "\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.dataset = df\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.loc[idx, \"text\"]\n",
    "        label = self.dataset.loc[idx, \"label\"]\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        sample = {\"text\": text, \"label\": label,\"input_ids\":input_ids,\"attention_mask\":attention_mask}\n",
    "        # print(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=pd.read_table('./data/train.txt',header=None)#text label\n",
    "data.columns = ['text', 'label']\n",
    "text=[i for i in data['text']]\n",
    "label=[i for i in data['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5356\n",
      "1    5356\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#可以通过df.colname 来指定某个列，value_count()在这里进行计数\n",
    "df2 = data.label.value_counts()  \n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text2token\n",
      "8570 2142\n",
      "DataLoader\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "name = 'other_score'\n",
    "data=pd.read_table('./data/'+name+'.txt',header=None)#text label\n",
    "data.columns = ['text', 'label']\n",
    "text=[i for i in data['text']]\n",
    "label=[i for i in data['label']]\n",
    "\n",
    "print('text2token')\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# added_token=['##char##']\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\",additional_special_tokens=added_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "def text2token(text,tokenizer,max_length=100):\n",
    "    text2id = tokenizer(\n",
    "        text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids=text2id[\"input_ids\"].tolist()\n",
    "    attention_mask=text2id[\"attention_mask\"].tolist()\n",
    "    return input_ids,attention_mask\n",
    "\n",
    "input_ids,attention_mask=text2token(text,tokenizer,max_length=100)\n",
    "\n",
    "data['input_ids']=input_ids\n",
    "data['attention_mask']=attention_mask\n",
    "\n",
    "train_data = data.sample(frac=0.8)\n",
    "test_data=data[~data.index.isin(train_data.index)]\n",
    "print(len(train_data),len(test_data))\n",
    "train_data=train_data.reset_index(drop=True)\n",
    "test_data=test_data.reset_index(drop=True)\n",
    "\n",
    "print('DataLoader')\n",
    "#按batch_size分\n",
    "\n",
    "\n",
    "batch_size=16\n",
    "train_loader = DataLoader(\n",
    "    SentimentDataset(train_data), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    SentimentDataset(test_data), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "import pickle\n",
    "with open(name+'_train_loader.pkl', 'wb') as f:\n",
    "    pickle.dump(train_loader, f)\n",
    "with open(name+'_test_loader.pkl', 'wb') as f:\n",
    "    pickle.dump(test_loader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device0 = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")#训练集gpu\n",
    "device1 = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")#测试集gpu\n",
    "import pickle\n",
    "with open(\"train_loader.pkl\",'rb') as f:\n",
    "    train_loader  = pickle.loads(f.read())\n",
    "with open(\"test_loader.pkl\",'rb') as f:\n",
    "    test_loader  = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(fn_cls, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"bert\")\n",
    "        self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "        self.model.to(device)\n",
    "        # self.dropout = nn.Dropout(0.3)\n",
    "        self.l1 = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.model(x, attention_mask=attention_mask)\n",
    "#         print(outputs[0])torch.Size([8, 100, 768])\n",
    "#         print(outputs[1])torch.Size([8, 768])\n",
    "#         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "        x = outputs[1]\n",
    "        # x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "# cls = fn_cls(device0)\n",
    "\n",
    "# from torch import optim\n",
    "# optimizer = optim.Adam(cls.parameters(), lr=1e-4)\n",
    "sigmoid = nn.Sigmoid()\n",
    "criterion = nn.BCELoss()#weight=weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(device_test):\n",
    "    cls.to(device_test)\n",
    "    cls.eval()\n",
    "\n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            label=batch['label'].to(device_test).float().view(-1,1)#batch size * 1\n",
    "            label_all.append(label)\n",
    "            input_ids=torch.stack(batch['input_ids']).t().to(device_test)#batch size * 100\n",
    "            attention_mask=torch.stack(batch['attention_mask']).t().to(device_test)#batch size * 100\n",
    "            \n",
    "            #计算输出\n",
    "            output = cls(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "            output=sigmoid(output)#batch size * 1\n",
    "            total+=len(output)\n",
    "            \n",
    "            #计算loss\n",
    "            loss = criterion(output, label)\n",
    "            epoch_loss+=loss\n",
    "            ave_loss=epoch_loss/total\n",
    "            \n",
    "            #四舍五入\n",
    "            output=output.round()\n",
    "            output_all.append(output)\n",
    "            \n",
    "            #计算准确率\n",
    "            add_correct=(output== label).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "            \n",
    "            if batch_idx%5==0:\n",
    "                print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，ave_loss：{}'.format(\n",
    "                    batch_idx, len(test_loader),100.*batch_idx/len(test_loader), \n",
    "                    correct, total,acc,\n",
    "                    ave_loss\n",
    "                    ),end= \"\\r\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    #结束：\n",
    "    print('正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "    \n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    output_all=torch.cat(output_all,0)\n",
    "    label_all=torch.cat(label_all,0)\n",
    "    \n",
    "    output_all=np.array(output_all.cpu())\n",
    "    label_all=np.array(label_all.cpu())\n",
    "    acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "    print(metrics.classification_report(label_all,output_all))\n",
    "    print(\"准确率:\",acc_score )\n",
    "    \n",
    "    return acc,epoch_loss.item()\n",
    "\n",
    "# test(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_l=[]\n",
    "train_epoch_loss_l=[]\n",
    "test_acc_l=[]\n",
    "test_epoch_loss_l=[]\n",
    "\n",
    "def train_one_epoch(device_train,epoch_num):\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"_______________\",epoch_num,\"start_______________\")\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"______________________________________________\")\n",
    "    cls.to(device_train)\n",
    "    cls.train()\n",
    "\n",
    "    epoch_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "    output_all=[]\n",
    "    label_all=[]\n",
    "    for batch_idx,batch in enumerate(train_loader):\n",
    "        label=batch['label'].to(device_train).float().view(-1,1)#batch size * 1\n",
    "        input_ids=torch.stack(batch['input_ids']).t().to(device_train)#batch size * 100\n",
    "        attention_mask=torch.stack(batch['attention_mask']).t().to(device_train)#batch size * 100\n",
    "\n",
    "        #计算输出\n",
    "        output = cls(input_ids, attention_mask=attention_mask)#batch size * 1\n",
    "        output=sigmoid(output)#batch size * 1\n",
    "\n",
    "        #计算loss\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            #四舍五入\n",
    "            output=output.round()\n",
    "            output_all.append(output)\n",
    "            label_all.append(label)\n",
    "            total+=len(output)\n",
    "            \n",
    "            #epoch_loss\n",
    "            epoch_loss+=loss\n",
    "            ave_loss=epoch_loss/total\n",
    "            \n",
    "            #计算准确率\n",
    "            add_correct=(output== label).sum().item()\n",
    "            correct+=add_correct\n",
    "            acc=correct/total\n",
    "            \n",
    "            if batch_idx%5==0:\n",
    "                print('[{}/{} ({:.0f}%)]\\t正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，ave_loss：{}'.format(\n",
    "                    batch_idx, len(train_loader),100.*batch_idx/len(train_loader), \n",
    "                    correct, total,acc,\n",
    "                    ave_loss\n",
    "                    ),end= \"\\r\")\n",
    "            \n",
    "            \n",
    "            \n",
    "    #结束：\n",
    "    print('正确分类的样本数：{}，样本总数：{}，准确率：{:.2f}%，ave_loss：{}'.format(\n",
    "                    correct, total,acc,\n",
    "                    ave_loss))\n",
    "    \n",
    "#     can't convert cuda:5 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "    with torch.no_grad():\n",
    "        output_all=torch.cat(output_all,0)\n",
    "        label_all=torch.cat(label_all,0)\n",
    "\n",
    "        output_all=np.array(output_all.cpu())\n",
    "        label_all=np.array(label_all.cpu())\n",
    "        acc_score=metrics.accuracy_score(label_all,output_all)\n",
    "        \n",
    "    # print(metrics.classification_report(label_all,output_all))\n",
    "    # print(\"准确率:\",acc_score )\n",
    "    \n",
    "    test_acc,test_epoch_loss=test(device1)\n",
    "    print('train_acc:',acc,'train_epoch_loss:',epoch_loss.item(),'test_acc:',test_acc,'test_epoch_loss:',test_epoch_loss)\n",
    "    train_acc_l.append(acc)\n",
    "    train_epoch_loss_l.append(epoch_loss.item())\n",
    "    test_acc_l.append(test_acc)\n",
    "    test_epoch_loss_l.append(test_epoch_loss)\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"_______________\",epoch_num,\"end_______________\")\n",
    "    print(\"______________________________________________\")\n",
    "    print(\"______________________________________________\")\n",
    "    return test_epoch_loss\n",
    "\n",
    "    \n",
    "# train_one_epoch(device0,0)\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "cls = fn_cls(device0)\n",
    "\n",
    "from torch import optim\n",
    "# cls=torch.load(\"./data/yxl_best.model\",map_location=device0)\n",
    "optimizer = optim.Adam(cls.parameters(), lr=1e-4)\n",
    "test(device1)\n",
    "now_loss = 999\n",
    "pre_epoch_loss = 9999\n",
    "epoch = 0\n",
    "while now_loss < pre_epoch_loss :\n",
    "    torch.save(cls,\"./data/yxl_best.model\")\n",
    "    pre_epoch_loss = now_loss \n",
    "    now_loss = train_one_epoch(device0,epoch)\n",
    "    epoch += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device,s_l,cls):\n",
    "    with torch.no_grad():\n",
    "        cls.to(device)\n",
    "        cls.eval()\n",
    "        text2id = tokenizer(\n",
    "            s_l, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids=text2id[\"input_ids\"].to(device)\n",
    "        mask=text2id[\"attention_mask\"].to(device)\n",
    "        output = cls(input_ids, attention_mask=mask)\n",
    "        output1=sigmoid(output)\n",
    "        output2=output1.round()\n",
    "        return output1,output2\n",
    "from tqdm import tqdm\n",
    "def run(device, s_l, cls, bs):\n",
    "    with torch.no_grad():\n",
    "        cls.to(device)\n",
    "        cls.eval()\n",
    "        len_ = len(s_l)\n",
    "        all_end_lgs = []\n",
    "        all_end = []\n",
    "        for start in tqdm(range(0, len_, bs)):\n",
    "            li_i = s_l[start:min(start+bs, len_)]\n",
    "            text2id = tokenizer(\n",
    "                li_i, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "                )\n",
    "            input_ids=text2id[\"input_ids\"].to(device)\n",
    "            mask=text2id[\"attention_mask\"].to(device)\n",
    "            output = cls(input_ids, attention_mask=mask)\n",
    "            output1=sigmoid(output)\n",
    "            output2=output1.round()\n",
    "            all_end_lgs = all_end_lgs + output1.tolist()\n",
    "            all_end = all_end + output2.tolist()\n",
    "    return all_end,all_end_lgs\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ['好好好好好好好',\n",
    "'坏坏坏坏坏坏坏坏',]\n",
    "print(predict(device1,s,cls)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d782eb80c6793658ece1c868de366382d6780e86e088d2fb13668bd1d900b34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
